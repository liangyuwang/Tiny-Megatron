# Tiny-Megatron

**Tiny-Megatron** is a minimalistic, educational re-implementation of the [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) library for distributed deep learning. This project provides clean, understandable implementations of various parallelism strategies used in large-scale language model training.

## ğŸš€ Features

### Multiple Parallelism Strategies
- **Tensor Parallelism (TP)**: Split individual layers across multiple devices
- **Data Parallelism (DP)**: Replicate model across devices, shard data batches  
- **2D Hybrid Parallelism**: Combine TP + DP for effective scalability

### Core Components
- **Custom Neural Network Modules**: Optimized implementations of Linear, Embedding, LayerNorm
- **Automatic Kernel Selection**: Runtime auto-tuner for optimal performance
- **Flexible Parallel Context**: Easy configuration of multi-dimensional parallelism
- **Wrapper-Based Design**: Non-intrusive parallelization of existing models

### Educational Focus
- **Clean, Readable Code**: Well-documented implementations for learning
- **Modular Architecture**: Each parallelism strategy is independently implemented
- **Complete Examples**: Full training scripts demonstrating each approach

## ğŸ“ Project Structure

```
Tiny-Megatron/
â”œâ”€â”€ tiny_megatron/core/             # ğŸ—ï¸ Core Library
â”‚   â”œâ”€â”€ dist/                       # Distributed Parallelism
â”‚   â”‚   â”œâ”€â”€ tp/                     # â€¢ Tensor Parallelism (TP)
â”‚   â”‚   â”œâ”€â”€ dp/                     # â€¢ Data Parallelism (DP)
â”‚   â”‚   â”œâ”€â”€ hybrid/                 # â€¢ 2D Hybrid Parallelism (TP + DP)
â”‚   â”‚   â””â”€â”€ utils/                  # â€¢ Communication utilities
â”‚   â”œâ”€â”€ module/                     # Custom NN Modules
â”‚   â”‚   â”œâ”€â”€ linear.py               # â€¢ Optimized Linear layers
â”‚   â”‚   â”œâ”€â”€ embedding.py            # â€¢ Embedding layers  
â”‚   â”‚   â”œâ”€â”€ normalization.py        # â€¢ LayerNorm implementation
â”‚   â”‚   â””â”€â”€ ops/                    # â€¢ Low-level operations
â”‚   â””â”€â”€ autotuner/                  # Performance Optimization
â”‚       â””â”€â”€ runtime_tuner.py        # â€¢ Automatic kernel selection
â”‚
â”œâ”€â”€ example/                        # ğŸš€ Training Examples
â”‚   â”œâ”€â”€ model.py                    # â€¢ GPT-2 model implementation
â”‚   â”œâ”€â”€ tp/train.py                 # â€¢ Tensor parallelism demo
â”‚   â”œâ”€â”€ dp/train.py                 # â€¢ Data parallelism demo  
â”‚   â””â”€â”€ hybrid/train.py             # â€¢ 2D hybrid parallelism demo
```

### ğŸ¯ Key Components

| Component | Purpose | Key Files |
|-----------|---------|-----------|
| **Distributed Parallelism** | Core parallel strategies | `dist/{tp,dp,hybrid}/` |
| **Custom Modules** | Optimized NN building blocks | `module/{linear,embedding}.py` |
| **ParallelContext** | Multi-dimensional coordination | `dist/utils/comm.py` |
| **Auto-tuner** | Performance optimization | `autotuner/runtime_tuner.py` |
| **Examples** | Complete training demos | `example/{tp,dp,hybrid}/` |

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8+
- PyTorch 2.0+ with CUDA support
- NCCL for multi-GPU communication

### Setup
```bash
git clone https://github.com/liangyuwang/Tiny-Megatron.git
cd Tiny-Megatron
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install tqdm
```

## ğŸ¯ Quick Start

### 1. Tensor Parallelism (2 GPUs)
```bash
# Split model layers across 2 GPUs
torchrun --nproc_per_node=2 example/tp/train.py
```

### 2. Data Parallelism (2 GPUs)  
```bash
# Replicate model, distribute data batches
torchrun --nproc_per_node=2 example/dp/train.py
```

### 3. 2D Hybrid Parallelism (4 GPUs)
```bash
# Combine TP and DP: TP=2 x DP=2
torchrun --nproc_per_node=4 example/hybrid/train.py
```

## ğŸ’¡ Usage Examples

### Basic Tensor Parallelism
```python
import torch
from tiny_megatron.core import ParallelContext, apply_tensor_parallel
from example.model import GPT2Model, GPTConfig

# Initialize distributed environment
# ... (distribution setup code)

# Create model and parallel context
config = GPTConfig()
model = GPT2Model(config).cuda()

# Configure parallelism
parallel_config = {"tp": 2}  # Use 2 GPUs for tensor parallelism
context = ParallelContext(parallel_config)

# Apply tensor parallelism
tp_config = {
    "column_linear_patterns": ["attn.c_attn", "mlp.c_fc"],
    "row_linear_patterns": ["attn.c_proj", "mlp.c_proj"]
}
model = apply_tensor_parallel(
    model=model, 
    parallel_context=context,
    tp_config=tp_config
)

# Train normally
optimizer = torch.optim.AdamW(model.parameters())
for batch in dataloader:
    loss = model(batch)
    loss.backward()
    optimizer.step()
```

### 2D Hybrid Parallelism
```python
from tiny_megatron.core import ParallelContext, apply_hybrid_parallel

# Configure 2D parallelism for 4 GPUs
parallel_config = {
    "tp": 2,  # 2-way tensor parallelism  
    "dp": 2   # 2-way data parallelism
}

context = ParallelContext(parallel_config)

# Apply 2D hybrid parallelism
tp_config = {
    "column_linear_patterns": ["attn.c_attn", "mlp.c_fc"],
    "row_linear_patterns": ["attn.c_proj", "mlp.c_proj"]
}
model = apply_hybrid_parallel(
    model=model,
    parallel_context=context,
    tp_config=tp_config
)
```

## ğŸ—ï¸ Architecture Overview

### Parallelism Strategies

#### Tensor Parallelism (TP)
- **Column Parallel**: Split weight matrices column-wise (e.g., attention projections)
- **Row Parallel**: Split weight matrices row-wise (e.g., MLP layers)
- **Communication**: All-gather for activations, all-reduce for gradients

#### Data Parallelism (DP)  
- **Model Replication**: Same model on each device
- **Data Sharding**: Different data batches per device
- **Gradient Synchronization**: All-reduce after backward pass

#### 2D Hybrid Parallelism
- **Combined Strategy**: Tensor Parallelism (TP) + Data Parallelism (DP)
- **Flexible Configuration**: Support various TP and DP combinations
- **Efficient Scaling**: Optimal resource utilization for medium-scale training

### Key Components

#### ParallelContext
Central coordination for multi-dimensional parallelism:
```python
context = ParallelContext({
    "tp": tensor_parallel_size,
    "dp": data_parallel_size
})
```

#### Custom Modules
Optimized implementations with built-in parallelism support:
- `Linear`: Matrix multiplication with automatic kernel selection
- `Embedding`: Token/position embeddings
- `LayerNorm`: Layer normalization

#### Runtime Auto-Tuner
Automatic selection of optimal kernels:
```python
tuner = RuntimeAutoTuner(
    warmup_iterations=10,
    measure_iterations=100
)
```

## ğŸ”§ Configuration

### Environment Variables
```bash
export MASTER_ADDR=localhost
export MASTER_PORT=29500
export WORLD_SIZE=4
export LOCAL_RANK=0
```

### Parallel Configuration
```python
parallel_config = {
    "tp": 2,    # Tensor parallel size
    "dp": 2,    # Data parallel size
}
```

## ğŸ“š Examples

Each parallelism strategy includes complete training examples:

- **`example/tp/train.py`**: Tensor parallelism with GPT-2
- **`example/dp/train.py`**: Data parallelism training
- **`example/hybrid/train.py`**: 2D hybrid parallelism demo

## ğŸ›£ï¸ Roadmap

### Currently Supported
- âœ… **Tensor Parallelism (TP)**: Column and row parallelism for linear layers
- âœ… **Data Parallelism (DP)**: Standard gradient synchronization
- âœ… **2D Hybrid Parallelism**: TP + DP combinations

### Future Plans
To maintain code simplicity and readability, we are currently focusing on TP and DP implementations. Future releases will include:

- ğŸ”„ **Pipeline Parallelism (PP)**: Layer-wise model partitioning
- ğŸ”„ **ZeRO Optimizer States**: Memory-efficient optimizer state sharding
- ğŸ”„ **Expert Parallelism (EP)**: Mixture-of-experts model scaling
- ğŸ”„ **Sequence Parallelism (SP)**: Sequence dimension parallelism for long contexts
- ğŸ”„ **5D Hybrid Parallelism**: TP + EP + SP + DP (ZeRO) + PP combinations

These advanced strategies will be added incrementally while maintaining the educational and minimalistic nature of the codebase.

## ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## ğŸ”— Related Projects

- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM): Original Megatron library
- [Tiny-FSDP](https://github.com/liangyuwang/Tiny-FSDP): Minimalistic PyTorch FSDP re-implementation
- [Tiny-DeepSpeed](https://github.com/liangyuwang/Tiny-DeepSpeed): Minimalistic DeepSpeed re-implementation

## ğŸ“– Citation

If you use Tiny-Megatron in your research, please cite:

```bibtex
@misc{tiny-megatron,
    title={Tiny-Megatron: A Minimalistic Re-implementation of Megatron-LM},
    author={Liangyu Wang},
    year={2024},
    url={https://github.com/liangyuwang/Tiny-Megatron}
}
```
